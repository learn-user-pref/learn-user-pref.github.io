<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning User Preferences for Image Generation Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon2.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <!-- 引入 MathJax 用于渲染 LaTeX 数学公式 -->
  <script type="text/javascript"
    id="MathJax-script"
    async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Learning User Preferences for Image Generation Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mowenyii.github.io/">Wenyi Mo</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/BarretBa">Ying Ba</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/tyz1994">Tianyu Zhang</a><sup>2*</sup>,
            </span>
            <span class="author-block">
              <a href="http://ylbai.me/">Yalong Bai</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://learn-user-pref.github.io/">Biye Li</a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Renmin University of China,</span>
            <span class="author-block"><sup>2</sup>iN2X</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
      
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Mowenyii/learn-user-pref"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/wenyii/learn-user-pref/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data (coming soon)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">


    <div class="content has-text-centered">
      <img src="static/images/motivation.png" alt="" width="50%"/>
      <p> Figure 1: Our task aims to predict target images that align with users' tastes based on their history data.</span>
    </div>


    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>User preference prediction requires a comprehensive and accurate understanding of individual tastes. This includes both surface-level attributes, such as color and style, and deeper content-related aspects, such as themes and composition. However, existing methods typically rely on general human preferences or assume static user profiles, often neglecting individual variability and the dynamic, multifaceted nature of personal taste.   
          </p>
          <p>
          To address these limitations, we propose an approach built upon Multimodal Large Language Models, introducing contrastive preference loss and preference tokens to learn personalized user preferences from historical interactions. The contrastive preference loss is designed to effectively distinguish between user "likes" and "dislikes", while the learnable preference tokens capture shared interest representations among existing users, enabling the model to activate group-specific preferences and enhance consistency across similar users. 
          </p>
          <p>Extensive experiments demonstrate our model outperforms other methods in preference prediction accuracy, effectively identifying users with similar aesthetic inclinations and providing more precise guidance for generating images that align with individual tastes.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/method.png" alt="Overview of our MLLM-based preference learning framework." width="100%"/>
            <p> Figure 2: Overview of our MLLM-based preference learning framework.</span>
          </div>
              <p>
              (a) The visual encoder and text embedding module extract preference representations 
              \( x_u^{+/-} \) by processing the preference history 
              \( \mathcal{S} \) and a target item 
              \( z_{\text{pos/neg}} \).
              </p>

              <p>
              (b) The framework is trained using a base loss 
              \( L_{\text{base}} \) to predict preference labels, and a contrastive preference loss 
              \( L_{\text{CP}} \) that enhances separability between liked and disliked items. 
              Additionally, learnable preference tokens 
              \( P_v \) are introduced to model shared user interests.
              </p>

          </p>
      </div> 



              <div class="content has-text-justified">
          <div class="content has-text-centered">
            <img src="static/images/attn.png" alt="" width="100%"/>
            <p> Figure 3: Mining Similar Users via Attention Mechanism.</span>
          </div>
              <p>
(a) Attention scores \(\mathcal{A}\) represent interactions between preference tokens and target image tokens for individual users. 
        Each user has a unique reference history, and we concatenate the same target image to the input sequence across users. 
        For each user, the horizontal axis represents tokens from the target image, while the vertical axis represents the preference tokens. Each user has five different random re-orderings of reference images.   
              </p>

              <p>
(b) Examples of images liked (✓) or disliked ( × ) by each user.
              </p>
          </p>
      </div> 



        </div>
      </div>
    </div>

  </div>
</section>



<!-- RESULTS SECTION -->
<section class="hero is-light is-small">
  <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista">Experiment Results</h1>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">


        <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">User-Specific Preference Prediction</h2>
        <div class="content has-text-centered">
          <img src="static/images/acc.png" alt="data-composition" style="max-width: 100%;"/>
          <p>Table 1: Preference classification accuracy on pairwise comparisons between liked and disliked images.</p>
        </div>
      </div>
    </div>



    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <h2 class="title is-3">Personalizing Generation with User Preferences</h2>
        <div class="content has-text-centered">
          <img src="static/images/reno.png" alt="data-composition" style="max-width: 100%;"/>
          <p class="content has-text-justified">Figure 4: Qualitative comparison of text-to-image generation for three users. Each row shows user preferences (Ref-dislike/like) and generation results from our personalized preference model vs. image-text alignment (CLIP Score), aesthetic quality (Aesthetic Score), general human preference (ImageReward, PickScore), and personalized preference (ViPer) models. We generate images guided by preference model while incorporating both positive and negative user feedback. </p>
        </div>

          <div class="content has-text-centered">
          <img src="static/images/reno-user.png" alt="data-composition" style="max-width: 80%;"/>
          <p class="content has-text-justified">Figure 5: Human expert evaluation of generated images from different methods on SD1.5-Turbo.</p>
        </div>

      </div>
    </div>



  </div>
</section>





<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Learning User Preferences for Image Generation Models},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->




<footer class="footer">
  <div class="container">

    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
